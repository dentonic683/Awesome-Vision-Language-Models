# Awesome Vision Language Models

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of awesome Vision Language Models (VLMs), resources, and tools. This repository aims to provide a comprehensive overview of state-of-the-art models and their applications in the field of computer vision and natural language processing.

## Table of Contents

- [Introduction](#introduction)
- [Table of Vision Language Models](#table-of-vision-language-models)
- [Contributing](#contributing)
- [License](#license)

## Introduction

Vision Language Models (VLMs) integrate visual and textual data to perform a variety of tasks such as image captioning, visual question answering, and image-text matching. This repository provides an overview of notable VLMs, their features, and links to relevant resources.

## Table of Vision Language Models

| Model Name | Description | Published Year | GitHub Repo | Paper |
|------------|-------------|----------------|-------------|-------|
| CLIP       | Contrastive Language-Image Pre-Training, a method for learning visual concepts from natural language supervision. | 2021 | [OpenAI/CLIP](https://github.com/openai/CLIP) | [Paper](https://arxiv.org/abs/2103.00020) |
| ViLBERT    | Vision-and-Language BERT, a model that learns joint representations of image and text. | 2019 | [HuggingFace/transformers](https://github.com/huggingface/transformers) | [Paper](https://arxiv.org/abs/1908.02265) |
| VisualBERT | A model for vision-and-language pre-training and downstream tasks. | 2019 | [uclanlp/visualbert](https://github.com/uclanlp/visualbert) | [Paper](https://arxiv.org/abs/1908.03557) |
| DALL-E     | A model for generating images from textual descriptions using a transformer architecture. | 2021 | [OpenAI/DALL-E](https://github.com/openai/DALL-E) | [Paper](https://arxiv.org/abs/2102.12092) |
| LXMERT     | Learning Cross-Modality Encoder Representations from Transformers for vision-and-language tasks. | 2019 | [HuggingFace/transformers](https://github.com/huggingface/transformers) | [Paper](https://arxiv.org/abs/1908.07490) |
| ImageBERT  | Cross-modal pre-training with large-scale image-text data. | 2020 | [microsoft/ImageBERT](https://github.com/microsoft/ImageBERT) | [Paper](https://arxiv.org/abs/2001.07966) |
| UNITER     | UNiversal Image-TExt Representation Learning, a unified framework for learning vision-and-language representations. | 2019 | [ChenRocks/UNITER](https://github.com/ChenRocks/UNITER) | [Paper](https://arxiv.org/abs/1909.11740) |

## Contributing

Contributions are welcome! Please read the [contributing guidelines](CONTRIBUTING.md) first. You can help by:

- Adding new vision language models.
- Improving the descriptions and information of existing models.
- Sharing resources, papers, and tutorials.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
