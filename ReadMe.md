[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/Jack-bo1220/Awesome-Remote-Sensing-Foundation-Models/graphs/commit-activity)
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

# <p align=center>`Awesome Vision Language Models`</p>

A curated list of awesome open source Multimodal Large Language Models (MLLM) with a special emphasis on Visual/Vision langauge models (VLM). This repository tracks recent advances and open-source releases of top-performing vision-language models.

## ðŸ“¢ Latest Updates
:fire: Last Updated on 2024.06.10 :fire:

- **2024.6.04**: Added Vila.
- **2024.5.25**: Added Llava.




## Vision Language Models (Small <5B)

| Model Name | Published Year | GitHub | Model/Quants | Cognition/Perception | MMMU | MME | Llava-bench | VQA v2 | GQA | MMB | SEED | MM-vet|
|------------|-------------|----------------|-------------|-------|
| CLIP       | Contrastive Language-Image Pre-Training, a method for learning visual concepts from natural language supervision. | 2021 | [OpenAI/CLIP](https://github.com/openai/CLIP) | [Paper](https://arxiv.org/abs/2103.00020) |
| ViLBERT    | Vision-and-Language BERT, a model that learns joint representations of image and text. | 2019 | [HuggingFace/transformers](https://github.com/huggingface/transformers) | [Paper](https://arxiv.org/abs/1908.02265) |
| VisualBERT | A model for vision-and-language pre-training and downstream tasks. | 2019 | [uclanlp/visualbert](https://github.com/uclanlp/visualbert) | [Paper](https://arxiv.org/abs/1908.03557) |
| DALL-E     | A model for generating images from textual descriptions using a transformer architecture. | 2021 | [OpenAI/DALL-E](https://github.com/openai/DALL-E) | [Paper](https://arxiv.org/abs/2102.12092) |
| LXMERT     | Learning Cross-Modality Encoder Representations from Transformers for vision-and-language tasks. | 2019 | [HuggingFace/transformers](https://github.com/huggingface/transformers) | [Paper](https://arxiv.org/abs/1908.07490) |
| ImageBERT  | Cross-modal pre-training with large-scale image-text data. | 2020 | [microsoft/ImageBERT](https://github.com/microsoft/ImageBERT) | [Paper](https://arxiv.org/abs/2001.07966) |
| UNITER     | UNiversal Image-TExt Representation Learning, a unified framework for learning vision-and-language representations. | 2019 | [ChenRocks/UNITER](https://github.com/ChenRocks/UNITER) | [Paper](https://arxiv.org/abs/1909.11740) |

## Vision Language Models (Medium < 34B)

## Vision Language Models (Large > 34B)


## Contributing

Contributions are welcome! Please read the [contributing guidelines](CONTRIBUTING.md) first. You can help by:

- Adding new vision language models.
- Improving the descriptions and information of existing models.
- Sharing resources, papers, and tutorials.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
